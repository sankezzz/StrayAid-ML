{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# injury_classifier.py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_1\", \"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset2/dataset_2\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_3\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_4\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_5\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset_paths\": [\n",
    "         \"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_1\", \"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_3\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_4\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_5\",\"C:/Users/sanke/OneDrive/Desktop/The Stray help/dataset_6\"],\n",
    "    \"image_size\": 224,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 10,\n",
    "    \"num_workers\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"device\": \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch-native transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InjuryDataset(Dataset):\n",
    "    def __init__(self, split_dirs, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for dataset_path in split_dirs:\n",
    "            img_dir = os.path.join(dataset_path, \"images\")\n",
    "            label_dir = os.path.join(dataset_path, \"labels\")\n",
    "            \n",
    "            for img_file in os.listdir(img_dir):\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                label_path = os.path.join(label_dir, f\"{os.path.splitext(img_file)[0]}.txt\")\n",
    "                \n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        label = int(f.read().strip())\n",
    "                    self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image with PIL\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders():\n",
    "    train_dirs = [os.path.join(d, \"train\") for d in CONFIG[\"dataset_paths\"]]\n",
    "    val_dirs = [os.path.join(d, \"valid\") for d in CONFIG[\"dataset_paths\"]]\n",
    "    test_dirs = [os.path.join(d, \"test\") for d in CONFIG[\"dataset_paths\"]]\n",
    "\n",
    "    train_dataset = InjuryDataset(train_dirs, train_transform)\n",
    "    val_dataset = InjuryDataset(val_dirs, val_transform)\n",
    "    test_dataset = InjuryDataset(test_dirs, val_transform)\n",
    "\n",
    "    # Class balancing\n",
    "    class_counts = np.bincount([s[1] for s in train_dataset.samples])\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = [class_weights[int(s[1])] for s in train_dataset.samples]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        sampler=sampler,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InjuryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.backbone(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InjuryDataset(Dataset):\n",
    "    def __init__(self, split_dirs, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for dataset_path in split_dirs:\n",
    "            img_dir = os.path.join(dataset_path, \"images\")\n",
    "            label_dir = os.path.join(dataset_path, \"labels\")\n",
    "            \n",
    "            for img_file in os.listdir(img_dir):\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                label_path = os.path.join(label_dir, f\"{os.path.splitext(img_file)[0]}.txt\")\n",
    "                \n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        # Read all values and take the first one\n",
    "                        label_values = f.read().strip().split()\n",
    "                        label = int(label_values[0])  # Use only the first value\n",
    "                        \n",
    "                    self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image with PIL\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sanke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 212\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/212 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model = InjuryClassifier().to(CONFIG['device'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_loaders()\n",
    "    \n",
    "    print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "\n",
    "    best_auc = 0\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        print(f\"Starting epoch {epoch+1}\")\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = images.to(CONFIG['device'])\n",
    "            labels = labels.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            assert not torch.isnan(loss), \"Loss is NaN!\"\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(CONFIG['device'])\n",
    "                outputs = model(images).squeeze().cpu().numpy()\n",
    "                \n",
    "                val_preds.extend(outputs)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        val_acc = accuracy_score(val_labels, np.round(val_preds))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader.dataset):.4f}\")\n",
    "        print(f\"Val AUC: {val_auc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# Run the training function\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
